# [Week10 - Day1] Deep Learning 13 - CNN 2

## 1. CNN 사례연구
  - 영상 분류
    - ImageNet
      - 2.2만여 클래스에 대해 클래스별로 수백~수만장의 사진을 인터넷에서 수집하여 1500여만장의 사진 데이터셋을 구축
    - ILSVRC 대회
      - 1000가지 클래스에 대해 분류,검출,위치 지정 문제
      - 120만 장의 Train Set, 5만 장의 Valid Set, 15만장의 Test Set
      - AlexNet(2012) -> Clarifi(2013) -> GoogLeNet & VGGNet(2014) -> ResNet(2015)
    - 우승 모델은 코드와 학습된 가중치를 공개

### 1-1 AlexNet
  - 구조
    - 컨볼루션층 5개와 FC층 3개
    - 컨볼루션층 200만개, FC층 6500만개의 파라미터
      - FC층이 파라미터가 30배 이상 많음 -> CNN의 향후 방향이 FC층의 파라미터를 줄이는 쪽으로 발전
    - 당시 GPU 메모리 크기로 인해 2개의 GPU에 분할하여 학습
      - 3번 컨볼루션층에서 2개의 결과를 함께 사용
      - GPU 1 - 색과 관련 없는 특징 추출
      - GPU 2 - 색에 관련된 특징 추출
    - 컨볼루션 층의 보폭을 크게 하여 다운 샘플링
  - 성공 요인
    - 외적 요인
      - ImageNet의 대규모 사진 데이터
      - GPU를 통한 병렬처리
    - 내부 요인
      - ReLU 사용
      - 지역 반응 정규화 사용
        - 위치가 비슷한 데이터끼리의 정규화를 통해 값을 조절
      - 과잉적합 방지를 위한 여러 규제 기법 적용
        - Data Augmentation (크롭과 반전으로 2048배 증가)
        - Dropout
    - 테스트 단계의 앙상블
      - 입력 영상에 Augmentation을 적용하고 적용된 영상들의 예측 평균으로 최종 인식
      - 에러율 2~3% 감소

### 1-2 VGGNet
  - 핵심 아이디어
    - 3*3의 작은 커널
    - 깊은 신경망
    - 8 ~ 16층의 컨볼루션층
  - VGG16 -> Conv 13층 + FC 3층
  - 작은 커널의 이점
    - GoogLeNet의 인셉션 모듈처럼 깊은 신경망 구조에 영향
    - 큰 크기의 커널은 작은 커널 여러개로 분해 가능 -> 파라미터 수는 줄어들면서 신경망은 깊어짐
    - 1 * 1 커널
      - 실험은 했지만 채택 X -> 이후 GoogLeNet에서 사용
      - 차원통합, 차원 축소

### 1-3 GoogLeNet
  - 인셉션 모듈
    - 수용장의 다양한 특징을 추출하기 위해 NIN의 구조를 확장하여 복수의 병렬적인 컨볼루션 층을 가짐
    - NIN 구조 (Network In Network)
      - 기존 컨볼루션 연산을 MLPConv 연산으로 대체
        - 커널 대신 비선형 함수를 포함하는 MLP를 사용 -> 특징 추출에 유리
      - 신경망의 Micro Neural Network가 주어진 수용장의 특징을 추상화
      - 전역 평균 풀링 사용
        - MLPConv가 클래스 수 만큼 특징 맵을 생성하면, 특징 맵 각각을 평균하여 출력 -> 파라미터 수 감소
  - NIN 개념을 확장한 신경망
    - MLPConv 대신 4종류의 컨볼루션 연산 사용 -> 다양한 특징 추출
  - 인셉션 모듈 9개를 결합
    - FC층은 1개 -> 100만개의 파라미터
    - 보조분류기
      - 원 분류기의 오류 역전파 결과와 보조 분류기의 역전파 결과를 결합 -> 경사 소멸 문제 완화
      - 학습에는 도우미역, 평가에는 제거

### 1-4 ResNet
  - 잔류(잔차) 학습 개념을 이용해 성능 저하 회피, 층수를 늘림
    - **y** = \tau(**F**(**x**) + **x**), 잔차 : **F**(**x**)
  - 지름길 연결
    - 깊은 신경망의 최적화 가능
    - 깊어진 신경망으로 인해 정확도 개선
    - 경사 소멸 문제 해결
  - VGGNet과 비교
    - 동일
      - 3 * 3 커널
    - 차이점
      - 잔류학습
      - 전역 평균 풀링 -> FC층 제거
      - 배치 정규화 -> 드롭아웃 불필요

## 2. 생성 모델
  - 현실의 데이터 발생 분포 P<sub>data</sub>(**x**) -> 알아낼수 없음
  - P<sub>data</sub>(**x**)를 모방하는 모델 P<sub>model</sub>(**x**)

### 2-1 GAN
  - 사람을 상대로 진짜와 가짜를 구별하는 실험에서 MNIST 52.4% CIFAR-10 78.7%
  - 핵심
    - 생성기와 분별기의 대립 구도
      - G는 가짜 샘플을 생성
      - D는 진짜와 가짜를 구별
    - 목표 : 생성기의 샘플을 분별기가 구별하지 못하는 수준까지 학습