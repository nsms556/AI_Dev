# [Week10 - Day4] Online Session
  - 이재구 멘토님

## 1. 이번주 리뷰
  - 지능의 원천 -> 학습
  - 딥러닝의 특징
    - Representation
  - 신겸망이 제대로 동작하려면
    - 신경망을 쌓기
    - 데이터에 적합한 신경망 구조
  - 신경망의 재활용 -> 전이 학습
    - 학습율이 중요
    - 자기주도 학습 등을 가능하게 함
  - Full Connection VS Partial Connection
    - 인접 픽셀들이 멀리 있는 픽셀보다 정보를 더 많이 공유 -> Locality
  - 컴퓨터 비전
    - 고전적인 CV : 사람이 필터를 계산을 통해 정의하여 사용
    - CNN : 입력과 결과를 통해 필터를 탐색
  - CNN
    - Global Average Pooling을 통해 FC층읋 없애는 방향으로 발전 중
    - AlexNet
    - GoogLeNet
    - VGGNet
    - ResNet - Residual Learning (잔차 학습)
      - 아이덴티티 레이어 - Gradient Highway
    - Instance Segmentation
  - GAN
    - Generative Model (생성 모델)
    - Generator VS Discriminator의 대립 구도
  - Recurrent Neural Network (RNN)
    - 요소간의 문맥 의존성 파악
    - Many-to-Many
      - 단어 태깅
  - LSTM
    - RNN의 한계 : 연속되는 곱 연산 -> 경사 소멸 or 경사 폭발이 잦음
      - 경사 폭발 : 규제를 통해 해결
      - 경사 소멸 해결의 아이디어 -> 저장
    - 입력 게이트, 출력 게이트, 망각 게이트, 메모리 셀
      - 입력 게이트 : 입력의 영향력 조절
      - 출력 게이트 : 출력의 정도 조절
      - 망각 게이트 : 망각 여부 조절
      - 메모리 셀 : 이전의 중요 데이터가 희석되지 않도록 보관
  - 결국 규모로 밀어버리는게 장땡
    - 모델 : 용량을 겁나 크게해서 모든 데이터에 적합시키기
    - 데이터 : 오버피팅이 일어난다면 데이터가 부족한게 아닌가 생각해봅시다
      - 자기주도고 데이터 증강이고 결국 데이터가 부족한 놈들이 쓰는것

## 2. QnA
  - 데이터를 보고 전이학습을 쓰면 좋겠다 / 쓰면 안되겠다 하는 판단의 기준이 있을까요
    - 태스크가 동일하다면 전이학습 OK
    - 그러나 데이터가 달라진다면 성능 하락
    - 데이터가 유사하다면 OK
  - fc층에서 avg pooling을 써서 1대 1 대칭이 이루어진다고 하셧는데, fc층에는 linear층을 쓰지 않는다는 것일까요
    - 기존 Linear 3층
    - 현재 avg 풀링을 통해 Linear 1층으로 감소
  - 예전에는 연산량을 줄이려고 애썼다면 요즘은 하드웨어의 능력치를 믿고 연산량을 줄이기보다는 성능위주로 가고 있다고 생각하면 되나요? 아니면 지금도 연산량은 최대한 줄이려는 노력을 하나요?
    - 컴퓨팅 자원의 파워를 믿고 성능 최대로!
    - 모바일 등을 위해 성능을 뽑은후 최대한 경량화
  