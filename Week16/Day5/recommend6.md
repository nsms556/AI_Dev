# [Week16 - Day5] Recommendation System 6 - 정리

## 1. 유데미 추천 엔진
  - 1기
    - Batch
    - 지난 90일 간 방문했던 유저들과 조건이 충족되는 강의들의 페어에 대해 각종 확률 예측
      - EMPI, P, CPE, NPE
    - 데이터 처리, 모델 예측 계산 -> 하둡, 하이브
    - 정보를 캐싱 시스템에 저장, 유닛 랭킹과 유닛 내 순서 결정에 사용
    - 다른 마케팅에도 정보 사용
    - 개발 ~ 런칭 -> 8개월
    - 다양한 엔지니어링팀과 협업 -> 회사 문화가 중요
      - A/B 테스트 프레임워크 개발
      - 유저 이벤트 로깅 시스템 개발
    - 평가 지표와 객관적인 측정법
      - A/B 테스트 프레임워크
      - 지표와 객관적 측정없이 순조로운 런칭 X
    - 스택
      - 하둡/하이브
      - 자바로 메모리 기반 협업 필터링 구현
  - 2기
    - 계산을 실시간으로 변경
      - 배치 추천에 비해 많은 장점
        - 유데미의 모든 유저에게 제공
        - 유저가 로그인하거나 홈페이지를 방문하면 추천 시작
      - 시스템 복잡도 증가
        - Kafka, Cassandra 등 메세지큐와 NoSQL 사용
      - 각종 모델 예측 기능을 API로 노출
    - 일부 특징은 배치 계산
      - 하둡, Spark
      - 해당 특징은 Cassandra에 저장

## 2. 추천 엔진 평가
  - 머신 러닝 모델 평가
    - 평가 지표 결정
      - 혼동행렬, AUC-ROC, F1 점수
      - RMSE, MAE, Log Loss
    - 평가 방법 결정
      - 홀드 아웃 테스트 (Train & Validation)
      - 교차 검증 (Cross Validation, K-Fold Test)
        - 홀드 아웃 보다 과적합 문제 감소
    - 특수한 교차 검증 : Leave One Out Cross Validation (LOOCV)
      - 교차 검증에서 폴드 수가 트레이닝 레코드 수와 동일한 경우
      - 테스트를 한 레코드로 진행 -> 시간이 오래걸림
  - 추천 엔진 평가
    - 평점 기반
      - 평점 예측하고 실제 평점과 비교
      - 추천 엔진에서는 RMSE, MAE 사용
    - Top-N 추천 정확도 기반
      - 유저별로 높은 평점 레코드 일부를 분리하여 나중에 추천되는 아이템들과 일치율 계산
      - LOOCV와 병행
        - scikit-learn과 surprise에서 지원
          - `from surprise.model_selection import LeaveOneOut`
        - 유저별로 하나의 평점만 출력
      - 모델 방식 추천
        - 평점 데이터에서 특정 유저의 모든 데이터 탐색
        - 한 레코드만 분리하여 테스트 셋에 추가
        - 나머지를 학습 데이터 셋에 추가
      - 만들어진 학습데이터로 모델 학습
      - 학습에 사용되지 않은 레코드들을 통해 평점 예측
        - 평점 정보가 없는 모든 유저ID와 아이템 ID
      - 유저별로 테스트 셋의 아이템 중 평점이 높은 것들 중에 추천된 Top-N개에 포함된 것의 비율을 계산하여 평균
      - 모든 유저들의 추천 정확도 평균
        - 추천 위치에 따라 가중치를 준다ㅇ면 이를 Top-N 추천 nDCG 정확도라 부름
  - 평가가 어려운 이유
    - 추천은 다양성이 필요
      - 비슷한 것만 추천하는 것은 문제가 됨
    - 상황에 따라 다른 추천이 가능한가
      - 사람의 취향은 변하며 상황에 따라 다른 것을 찾을 수도 있음
    - 순서를 고려할 수 있는가
      - 시리즈로 구성된 경우 시리즈의 첫번째 아이템을 추천하는 것이 좋음

## 3. A/B 테스트
  - 온라인 실험
  - 실제 유저를 대상으로 새로운 기능이나 변경을 객관적으로 검증
  - 테스트 전 지표를 정해 테스트의 성공 여부를 결정
  - 한번에 하나의 새로운 기능, 변화를 테스트
    - 2가지 이상이 되면 결과를 해석할 수 없음
  - 작은 수의 유저들에게 먼저 노출시켜 위험부담 감소
  - 인프라가 없으면 테스트 불가능
    - 모든 엔지니어링 팀과 협업 필요
  - 과정  
    - 가설 세우기
    - 유저들을 같은 크기와 같은 속성의 두 그룹으로 분리
      - 기존 기능 vs 새로운 기능
    - 유저들의 행동 기록
    - 그룹 별로 지표 계산, 비교
      - 지표 차이가 통계적으로 유의미한지 확인
      - 시간에 따라 흐름을 확인

## 4. 개인정보 이슈
  - 개인정보
    - 개인을 식별할 수 있는 정보
      - 특정 개인을 알아보기 어려운 경우는 제외
      - 이름, 이메일, 전화번호, 주소 등
    - 준식별자
      - 몇가지 조합으로 개인을 식별할 수 있는경우
  - 개인 정보 보호
    - 개인의 정보가 적절한 동의 없이 노출, 배포되지 않은 것
    - 관련 법안
      - 국내 : 개인정보보호법, 정보통신망법, 클라우드 컴퓨팅법
      - 미국 : CCPA, HIPPA
      - EU : GDPR
    - GDPR
      - 대상
        - EU 회원국 전체에 일괄적용
        - EU 회원국의 유저가 있는 웹서비스도 적용 대상
      - 위반시 과징금 등 행정처분
      - 세부사항
        - 아동정보에 대해 더 강한 보호
        - 민감정보는 원칙금지
        - 정보주체의 권리 강화
      - 프로파일링 거부권
        - 프로파일링
          - 개인의 경제 상황, 관심, 행동 등을 다양한 데이터 기반으로 자동 분석, 예측
          - 특정인에 대한 낙인, 차별, 감시, 정보선택권 제한 등 새로운 프라이버시 위험요소가 될 수 있음
        - 프라이버시 존중 검색 엔진 등장
          - 덕덕고

## 5. 교훈
  - 주의점
    - Cold Start (특히 협업 필터링 기반 추천)
      - 유저 데이터 : 유저가 서비스를 처음 사용하는 경우
      - 아이템 데이터 : 아이템이 처음 서비스에 노출되기 시작하는 경우
    - 확장성
      - 수 많은 유저의 처리
      - 수 많은 아이템의 처리
      - 모델링에 걸리는 시간 외에 서빙 시 시간도 중요
  - 고려할 점
    - 데이터의 부족
      - 명시적, 암시적 레이블 데이터 부족
        - 리뷰/평점 데이터와 클릭 데이터의 크기는 상대적으로 작음
        - 평점 데이터는 조작 가능
    - 다양한 아이템의 추천
      - 관점 혹은 선호도 편향화 심화
      - 새로운 아이템을 어떻게 노출시킬지 고민 필요
    - 인프라 필요
      - 필요한 데이터가 수집, 저장되어야 하고 이를 처리할 인프라가 필요
      - 인프라 없이 지속적인 개발, 개선 불가능
      - Spark가 많이 사용됨
    - 개인정보 보호
      - 많은 추천이 개인정보들을 취합하여 만들어짐
        - GDPR, CCPA 등 법률 존재
      - 의도치 않은 개인정보 노출 가능
        - 검색어 자동완성