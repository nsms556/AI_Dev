# [Week15 - Day4] Online Session
  - 한기용 멘토님
  - 추천 시스템

## 추천 엔진
  - 상품을 고객에게 보여줬을 때 구매할 확률이 높은 것들을 보여주는 것
  - 충분한 데이터가 쌓이면 구매 패턴을 보고 물건 서비스 추천 가능
    - 협업 필터링
  - Explore & Exploit 알고리즘
    - 구글 키워드 광고
  - 유데미 실제 사용시 반대 의견
    - 어디서든 반대가 생기기 마련
    - A/B 테스트 등을 통해 객관적인 지표를 만드는 것이 필요

## 모델 개발시 공통적인 문제
  - 트레이닝 셋 관리
    - 수집 - labeled 데이터 비율, bias 유무
    - 보관 및 관리
    - Feature 추출 - 유닛 테스트 통한 디버깅, 개발 시간 단축
  - ML 모델 빌딩과 검증
    - 학습 알고리즘 + 하이퍼 파라미터 테스트 자동화
  - 모델 관리
    - 모델 별 알고리즘/하이퍼파라미터/트레이닝 셋을 유기적으로 보관
    - 모델 재연성이 중요해짐
    - 버전 관리가 필요해짐
  - 런칭 프로세스
    - 모델 런칭 방법
    - 프로덕션 엔지니어링 팀과 협업 필요
  - AB 테스트 프로세스
    - 실제 유저의 반응을 통한 검증
    - 테스트 프레임워크 필요
      - 해결하려는 문제, 왜
      - 지표 측정

## AWS SageMaker
  - 머신러닝 모델 개발을 위한 AWS 프레임워크
  - 기능
    - 트레이닝 셋 준비
    - 모델 학습
    - 검증
    - 배포 및 관리
  - 다양한 프레임 워크 지원
    - TensorFlow, PyTorch, MXNet
    - 자체 SageMaker 모듈로 모델 학습 가능
  - SageMaker Studio 웹기반 환경
  - 다양한 개발방식
    - 기본은 Python Notebook
      - 스칼라/자바 SDK 제공
    - AutoPilot

## QnA
  - 추천시스템 구현에 필요한 데이터는 무엇인가요?
    - 아이템 프로파일, 유저 프로파일 -> 구매 이력등이 있으면 유사도 계산 문제로 풀이 가능
  - 추천 시스템 구현은 어떠한 방식으로 score를 매기나요??
    - 아이템에 대한 유저의 반응 기록 필요
    - 일반 머신러닝 모델보다 더욱 복잡한 문제
  - 막 출시된 서비스라 유저가 부족한 경우는 collaborative filtering을 사용할 수 없나요?
    - 불가능 -> Cold Start 이슈
  - 만들어진 모델을 배포하는 과정을 협엽하기 위해서는 어떤 거를 공부해야 할까요???
    - DevOps 직군의 주 업무
    - 서비스 모니터링, 모델 전개에 관한 문제
  - 추천모델을 만들었을때는 속도가 중요하지는 않을것 같지만, 속도가 필요한 모델의 경우, 어떠한 과정을 거쳐서 최적화가 되는지도 궁금합니다.
    - 서비스 유저가 매우 많아지면 속도가 중요해짐
    - 유저가 많아지면서 처리할 데이터량이 증가 -> 컴퓨팅 자원의 한계로 시간이 늘어남
  - titanic data를 가지고 sagemaker 사용했을 때, 얼마나 과금 되나요?
    - 안해봐서 모름...
    - 비싼 인스턴스, 근데 프리티어 적용
