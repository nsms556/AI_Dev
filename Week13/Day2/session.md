# [Week13 - Day2] Online Session
  - 강창성 멘토님

## 검색에서의 인공지능
### 검색엔진
  - 1세대 
    - 통사적 유사성
  - 2세대 
    - 페이지 내의 정보 이상을 사용
    - 링크 분석, 클릭
  - 3세대
    - 10 blue links 넘기
    - 검색결과의 개인화, 직접적인 답변
  - 4세대
    - 딥러닝 기반 속성, 랭킹

### 검색과학
  - 랭킹 적절성
  - 검색어 분석
  - 컨텐츠 분석
  - 전체 페이지 적절성

### 랭킹함수
  - 일반적인 학습
    - 검색로그에서 유저의 검색어 표본 추출
    - (Query, URL)에 대한 평가 (PEFGB 5단계)
    - (Query, URL)을 속성벡터로 표기
    - 랭킹함수 학습
  - 평가
    - 오프라인
      - 랭킹함수를 사용해서 테스트 데이터를 생성
      - 모든 (query, URL)을 평가
      - 평가지표를 사용해서 함수 간 비교
    - 온라인
      - A/B 테스트를 통해 유저의 만족도 측정
      - 클릭이 많아지는가, 검색어를 얼마나 자주 재구성하는가
      - 충돌이 있는 다른 지표들을 고려해서 종합적 판단
  - 속성
    - 문서 의존
      - 웹그래프 관련 속성
      - 자동 문서분류 결과
    - 검색어 의존
      - 검색어 의도 분류
    - 검색어-문서 의존
      - 통사적 텍스트 매칭
      - 의미적 텍스트 매칭
    - 사용자 행동
      - 클릭
  - 머신러닝
    - pointwise
    - pair-wise
    - List-wise
      - DCG 같은 평가지표를 미분 가능 문제로 바꾸어 최적화
    - 머신러닝이 필요한 이유
      - 규칙기반 대비 장점
        - 빈번하지 않거나 새로운 검색어
        - 빠른 개발 사이클
        - 확장성
        - 랭킹이 몇명에게 제한되지 않음
      - 단점
        - 결과에 대한 설명이 어려움
        - 특정상황에서 변화가 어려움
  - 검색어 다시쓰기
    - 문서번역을 위한 머신러닝 적용
      - 검색어 언어를 문서 언어로 번역
      - Seq2seq 방법 사용
  - 의미적 텍스트 매칭
    - 의미적 유사성 함수
    - 속성
      - 클릭을 사용해 검색어, 문서표현
        - 유저의 검색결과에 대한 행동을 사용해 검색어와 문서를 동일한 공간으로 투영
        - 동일 공간에서 표햔된 검색어-문서 사이의 유사성을 속성으로 사용
      - 기계번역 기반
        - 기계번역 모델을 사용해서 검색어를 여러가지로 변환
        - 유사성 계산
      - 딥러닝 기반
        - 유사데이터와 비유사 데이터를 수집
        - 딥러닝 모델을 통해 검색어와 문서의 표현, 유사성함수를 학습

### 요약
  - 검색엔진에서의 랭킹
    - 유저로부터 검색어가 주어지면 기계가 이해하기 쉬운 데이터로 변환
  - 랭킹 향상
    - 대용량의 학습 데이터
    - 검색어 이해를 위한 분류모델
    - 의미적 텍스트 매칭
    - 딥러닝 모델이 좋은 성능을 보여줌

## QnA
  - 최근 검색 결과에 따라 검색 내용이 달라지는것도 학습에서 사용되는건가요
    - 최근 검색 역시 머신러닝의 입력으로 다시 사용 -> 결과가 달라짐
  - 단순히 맞춤법에 대한 문제가 아닌 답변의 늬앙스가 맞는지에 대한 모델의 평가 방법은 사람만 가능할까요?
    - 자가지도학습 등으로 가능은 함
  - 추천시스템에서 협업필터링도 같은 키워드(검색어)를 찾는 사용자가 어떤 콘텐츠(사이트)에 공통적으로 수요(클릭)한다는 정보를 쓰는거 같은데 의미적 텍스트 매칭과 비슷하다고 볼 수 있을까요..?
    - 유사한 것으로 칠 수 있음
    - 검색어 대신 유저를 임베딩으로 하여 사용한다면 비슷함
  - 같은 시기에 서로 다른 컴퓨터에서 같은 검색을 시도했을때 내용이 다를 때
    - 유저에 따라 결과가 다르기에 가능 함
    - A/B 테스트에 따라 갈릴 수도

## NLP 과제
  - word2vec -> subword 임베딩
  - subword 단위에 주의
  - Subword tokenization 구현 X -> Sentence Piece에 포함된 BPE 사용
  - 학습
    - Negative Sampling을 사용해서 임베딩, subword에 대한 임베딩
    - Negative Sampling 사용 X, skip-gram을 Cross Entropy Loss를 사용한 Multi-Class Classification 문제로 가정하여 해결, 필요하다면 SentencePiece에서 vocabulary size 제한 가능